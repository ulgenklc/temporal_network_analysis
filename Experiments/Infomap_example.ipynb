{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from math import floor\n",
    "from elephant.spike_train_generation import homogeneous_poisson_process\n",
    "import elephant.conversion as conv\n",
    "import neo as n\n",
    "from quantities import Hz, s, ms\n",
    "\n",
    "from infomap import Infomap, MultilayerNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_cross_corr(x,y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    x_cov_std = np.nanmax(np.sqrt(np.correlate(x - x_mean, x - x_mean, 'full')))\n",
    "    y_cov_std = np.nanmax(np.sqrt(np.correlate(y - y_mean, y - y_mean, 'full')))\n",
    "\n",
    "    normalization = x_cov_std * y_cov_std\n",
    "        \n",
    "\n",
    "    unnormalized_correlation = np.correlate(x - x_mean, y - y_mean, 'full')\n",
    "    \n",
    "    corr_array = unnormalized_correlation/normalization\n",
    "\n",
    "    return(corr_array)\n",
    "\n",
    "def max_norm_cross_corr(x1, x2):\n",
    "    \n",
    "    correlation= normalized_cross_corr(x1, x2)\n",
    "    \n",
    "    lag = abs(correlation).argmax() - len(x1)+1\n",
    "    \n",
    "    max_corr = max(abs(correlation))\n",
    "    \n",
    "    return(max_corr, lag)\n",
    "\n",
    "def cross_correlation_matrix(data):\n",
    "    #input: n x t matrix where n is the number of rois and t is the duration of the time series\n",
    "    #return: n x n symmetric cross correlation matrix, nxn uppertriangular cross correlation matrix and lag matrix\n",
    "    n, t = data.shape\n",
    "    X = np.zeros((n,n))\n",
    "    lag = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        for j in range(i+1,n):\n",
    "            X[i][j],lag[i][j] = max_norm_cross_corr(data[i,:],data[j,:])\n",
    "    X[np.isnan(X)] = 0\n",
    "    lag[np.isnan(lag)] = 0\n",
    "    \n",
    "    X_full = X + X.T\n",
    "    lag = lag + lag.T\n",
    "    return(X_full, X, lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_time_series(array, binsize, gaussian = True, **kwargs):\n",
    "    #input: nxt matrix \n",
    "    #returns: binned time series i.e. l x n x binsize\n",
    "    \n",
    "    n = array.shape[0] # number of neurons\n",
    "    totalsize = array.shape[1] # total duration of spikes\n",
    "    gauss_array = np.zeros((n,totalsize))\n",
    "    l = int(totalsize/binsize) # number of resulting layers\n",
    "    if gaussian:\n",
    "        for i in range(n):\n",
    "            gauss_array[i] = gaussian_filter(array[i],kwargs['sigma'])\n",
    "    else: gauss_array = array\n",
    "    A = np.zeros((l,n,binsize))\n",
    "    for i in range(l):\n",
    "        A[i] = gauss_array[:,i*binsize:(i+1)*binsize]\n",
    "    return(A)\n",
    "\n",
    "def binarize(array):\n",
    "    n,t = array.shape\n",
    "    binary_spikes = np.zeros((n,t), dtype = bool)\n",
    "    for i in range(n):\n",
    "        for j in range(t):\n",
    "            if array[i][j] == 0: pass\n",
    "            else: binary_spikes[i][j] = True\n",
    "    return(binary_spikes)\n",
    "\n",
    "def gaussian_filter(array,sigma):\n",
    "    #sigma=0.25==gaussian kernel with length 3\n",
    "    #sigma=0.5==gaussian kernel with length 5\n",
    "    #sigma=1==gaussian kernel with length 9\n",
    "    return(gaussian_filter1d(array,sigma))\n",
    "\n",
    "def jitter(spike, k):\n",
    "    #jittering the given spike train\n",
    "    jittered = np.zeros(spike.shape)\n",
    "    for i in np.nonzero(spike)[1]:\n",
    "        jitt = random.randint(-k,k)\n",
    "        try:jittered[0,i+jitt] = 1\n",
    "        except:jittered[0,i] = 1\n",
    "    return(jittered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Create spike trains via a Poisson Process*.\n",
    "### * We define 2 layers and 2 and 4 communities of fixed size in layer 1 and layer 2 respectively. We jitter the master spike in each community(shift the spike right or left randomly with standard deviation of 5 frames) to create a syncronized neuronal community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_rate = [10, 30] # spike rate per layer i.e. every community in the respective layer has this spike rate\n",
    "comm_sizes = [[30,30],[15,15,15,15]] # community sizes at every layer\n",
    "num_neurons = int(sum(comm_sizes[0])) #total number of neurons\n",
    "bin_size = 1000.0 # in frames, in every bin_size, a community activity occurs(splitting, growing, merging etc..)\n",
    "seconds = len(spike_rate)\n",
    "total_duration = int(seconds*bin_size)\n",
    "window_size = 1000 # size, in frames, each adjacency matrix correspond to. better to be equal to bin_size \n",
    "standard_dev = 1.2 # for gaussian kernel\n",
    "k = 5 #for jittering the spikes\n",
    "layers = int(total_duration/window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = np.zeros((num_neurons, total_duration))\n",
    "for s in range(seconds):\n",
    "    neuron_count = 0\n",
    "    for i,e in enumerate(comm_sizes[s]):\n",
    "        initial_master = homogeneous_poisson_process(rate = spike_rate[s]*Hz, t_start = s*(bin_size)*ms, t_stop = (s+1)*bin_size*ms, as_array=True)\n",
    "        master_spikes = np.zeros((1,total_duration))\n",
    "    \n",
    "        for j,f in enumerate(initial_master):\n",
    "            master_spikes[0][int(f)] = 1\n",
    "\n",
    "        for j in range(e):\n",
    "            spikes[neuron_count+j][int(s*bin_size):int((s+1)*bin_size)] = jitter(master_spikes[:,int(s*bin_size):int((s+1)*bin_size)], k)\n",
    "        neuron_count = neuron_count + e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Display spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(20,10))\n",
    "ax.imshow(spikes, origin = 'lower', interpolation='nearest', aspect='auto',  extent = [0,total_duration,0,num_neurons])\n",
    "ax.set_title('Spike Trains generated via Poisson Process for %d synthetic neurons'%num_neurons, fontsize= 30)\n",
    "ax.set_xlabel('TIME (in Miliseconds)', fontsize = 20)\n",
    "ax.set_xticks([j*1000 for j in range(int(total_duration/1000)+1)])\n",
    "ax.set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "ax.set_ylabel('Neuron ID', fontsize = 25)\n",
    "ax.set_xlabel('Time (Frames)', fontsize = 20)\n",
    "ax.tick_params(axis = 'both', labelsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Divide the time series in half each having length 1000 frames & widen the spikes by multipyling each spike with a gaussian kernel with some standard deviation to increase the cross-correlation in 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_spikes = bin_time_series(spikes, window_size, gaussian = True, sigma = standard_dev)\n",
    "fig,ax = plt.subplots(layers,1,figsize=(15,10))\n",
    "for i in range(layers):\n",
    "    ax[i].imshow(binned_spikes[i], origin = 'lower', interpolation='nearest', aspect='auto')\n",
    "    ax[i].set_title('Gaussian Binned Spikes (Layer %d)'%(i+1), fontsize = 20)\n",
    "    ax[i].set_xlabel('TIME (in Miliseconds)', fontsize = 20)\n",
    "    ax[i].set_xticks([j*100 for j in range(11)])\n",
    "    ax[i].set_yticks([j*10 for j in range(int(num_neurons/10)+1)])\n",
    "    ax[i].set_ylabel('Neuron ID', fontsize = 25)\n",
    "    ax[i].set_xlabel('Time (Frames)', fontsize = 20)\n",
    "    ax[i].tick_params(axis = 'both', labelsize = 20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Calculate the maximum positive cross correlation betwen pairs of neurons to create adjacency matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create cross-correlation matrices that are the adjacency matrices of the network at each layer\n",
    "adjacency_matrices = []\n",
    "for i in range(layers):\n",
    "    adjacency_matrices.append(cross_correlation_matrix(binned_spikes[i])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Display adjacency matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,layers, figsize = (5*layers,4))\n",
    "for i in range(layers):\n",
    "    k = ax[i].imshow(adjacency_matrices[i], \n",
    "                            origin = 'lower', \n",
    "                            interpolation='nearest', \n",
    "                            aspect='auto',  \n",
    "                            extent = [0,num_neurons,0,num_neurons])\n",
    "    ax[i].set_title('Adjacency Matrix (Layer %d)'%(i +1), fontsize = 10)\n",
    "    ax[i].set_xticks([k*10 for k in range(int(num_neurons/10)+1)])\n",
    "    ax[i].set_yticks([k*10 for k in range(int(num_neurons/10)+1)])\n",
    "    ax[i].tick_params(axis = 'both', labelsize = 10)\n",
    "cbar = fig.colorbar(k, ax = ax.flat)\n",
    "cbar.ax.tick_params(labelsize = 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Add edges to the infomap object and run infomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Infomap(\"--two-level --directed\")\n",
    "for i,e in enumerate(adjacency_matrices):## list of length 2 corresponding to the adjacency matrices in each layer\n",
    "    for j,f in enumerate(e):\n",
    "        for k in range(len(f)):# f is each row of the adjacency matrix\n",
    "            s = MultilayerNode(layer_id = i, node_id = j)\n",
    "            t = MultilayerNode(layer_id = i, node_id = k)\n",
    "            \n",
    "            im.add_multilayer_link(s,t,f[k])\n",
    "\n",
    "##diagonal coupling with interlayer coupling strength 1\n",
    "for i in range(len(adjacency_matrices)-1):\n",
    "    for j in range(num_neurons):# number of nodes which is 60 in the multilayer network\n",
    "        s = MultilayerNode(layer_id = i, node_id = j)\n",
    "        t = MultilayerNode(layer_id = i+1, node_id = j)\n",
    "        im.add_multilayer_link(s, t, 0.0001)\n",
    "im.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {im.num_top_modules} modules with codelength: {im.codelength}\")\n",
    "\n",
    "print(\"\\n#layer_id node_id module_id:\")\n",
    "for node in im.nodes:\n",
    "    print(f\"{node.layer_id} {node.node_id} {node.module_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
