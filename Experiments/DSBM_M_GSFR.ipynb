{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.all import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helpers import *\n",
    "from Temporal_Community_Detection import temporal_network\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms1 = 6\n",
    "comms2 = 3\n",
    "\n",
    "layers = 2\n",
    "\n",
    "fixed_size = int(abs(np.random.normal(30,10)))\n",
    "\n",
    "fixed_rate1 = int(abs(np.random.normal(20,8)))\n",
    "fixed_rate2 = int(abs(np.random.normal(20,8)))# spike rate per commiunity\n",
    "\n",
    "path = '/projects/academic/smuldoon/bengieru/Community_Detection/M_FSFR/DSBM/'\n",
    "\n",
    "os.makedirs(path, exist_ok = True)\n",
    "\n",
    "comm_sizes = [[fixed_size for i in range(comms1)], #layer1 community sizes\n",
    "              [3*fixed_size,fixed_size,2*fixed_size]] #layer2 community sizes\n",
    "spike_rates = [[fixed_rate1 for i in range(comms1)],\n",
    "              [fixed_rate2 for i in range(comms2)]]\n",
    "\n",
    "with open(path + \"comm_size.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(comm_sizes, fp)\n",
    "\n",
    "window_size = 1000 # size, in frames, each adjacency matrix correspond to. better to be equal to bin_size \n",
    "standard_dev = 1.2 # for gaussian kernel\n",
    "k = 5 #for jittering the spikes\n",
    "pad = True\n",
    "num_neurons = int(sum(comm_sizes[0]))\n",
    "\n",
    "display_truth(comm_sizes, community_operation = 'merge')\n",
    "plt.savefig(path + 'Ground_truths.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spikes = create_time_series('merge', comm_sizes, spike_rates, windowsize = window_size, k = k)\n",
    "plt.savefig(path + 'spiketrain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + \"spikes.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(spikes, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 1, figsize = (10,10))\n",
    "n, bins = spike_count(spikes, ax)\n",
    "plt.savefig(path + 'spike_distribution.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_spikes = bin_time_series(spikes, window_size, gaussian = True, sigma = standard_dev)\n",
    "fig,ax = plt.subplots(layers, 1, figsize=(20,50))\n",
    "for i in range(layers):\n",
    "    ax[i].imshow(binned_spikes[i], origin = 'lower', interpolation='nearest', aspect='auto')\n",
    "    ax[i].set_title('Gaussian Spikes (Layer %d)'%(i+1), fontsize = 20)\n",
    "    ax[i].set_xlabel('TIME (in Miliseconds)', fontsize = 20)\n",
    "    ax[i].set_xticks([j*100 for j in range(11)])\n",
    "    ax[i].set_yticks([j*10 for j in range(int(num_neurons/10)+1)])\n",
    "    ax[i].set_ylabel('Neuron ID', fontsize = 25)\n",
    "    ax[i].set_xlabel('Time (Frames)', fontsize = 20)\n",
    "    ax[i].tick_params(axis = 'both', labelsize = 20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(path + 'binned_spiketrain.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create cross-correlation matrices that are the adjacency matrices of the network at each layer\n",
    "adjacency_matrices = []\n",
    "for i in range(layers):\n",
    "    adjacency_matrices.append(cross_correlation_matrix(binned_spikes[i])[0])\n",
    "    \n",
    "if pad:\n",
    "    padded_adjacencies = [adjacency_matrices[0]]  + adjacency_matrices + [adjacency_matrices[-1]]\n",
    "    layers = layers + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2, 2, figsize = (32,32))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        k = ax[i][j].imshow(padded_adjacencies[i*2+j], \n",
    "                            origin = 'lower', \n",
    "                            interpolation='nearest', \n",
    "                            aspect='auto',  \n",
    "                            extent = [0,num_neurons,0,num_neurons])\n",
    "        ax[i][j].set_title('Adjacency Matrix (Layer %d)'%(i*2+j +1), fontsize = 35)\n",
    "        ax[i][j].set_xticks([k*10 for k in range(int(num_neurons/10)+1)])\n",
    "        ax[i][j].set_yticks([k*10 for k in range(int(num_neurons/10)+1)])\n",
    "        ax[i][j].tick_params(axis = 'both', labelsize = 20)\n",
    "fig.suptitle('Community merge with sizes %d,%d,%d,%d,%d,%d and spike rates %d,%d,%d,%d,%d,%d for 1st layer and %d,%d,%d for 2nd layer'%(comm_sizes[0][0],comm_sizes[0][1],comm_sizes[0][2],comm_sizes[0][3],comm_sizes[0][4],comm_sizes[0][5],spike_rates[0][0],spike_rates[0][1],spike_rates[0][2],spike_rates[0][3],spike_rates[0][4],spike_rates[0][5],spike_rates[1][0],spike_rates[1][1],spike_rates[1][2]), fontsize = 45)\n",
    "cbar = fig.colorbar(k, ax = ax.flat, orientation = 'horizontal')\n",
    "cbar.ax.tick_params(labelsize = 25) \n",
    "plt.savefig(path+'adjacency.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = temporal_network(num_neurons, \n",
    "                      layers, \n",
    "                      window_size, \n",
    "                      data = 'list__adjacency', \n",
    "                      list_adjacency = padded_adjacencies, \n",
    "                      omega = 1, \n",
    "                      kind = 'ordinal')\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize = (25,15))\n",
    "TN.raster_plot(spikes, ax)\n",
    "plt.savefig(path + 'raster_plot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = 26\n",
    "threshs = np.linspace(0, 0.5, grid)\n",
    "\n",
    "path_deg_corr = path + 'deg_corr/'\n",
    "\n",
    "os.makedirs(path_deg_corr, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_matrices = {}\n",
    "for k, f in enumerate(threshs):\n",
    "    edge_lists = [[] for i in range(layers)]\n",
    "    for i in range(layers):\n",
    "        A = padded_adjacencies[i]\n",
    "        firing = np.transpose(np.nonzero(A))\n",
    "        for j,m in enumerate(firing):\n",
    "            if A[m[0],m[1]]<f: pass\n",
    "            else: \n",
    "                quadreplet =(m[0], m[1], A[m[0], m[1]], i)\n",
    "                edge_lists[i].append(quadreplet)\n",
    "    processed_matrices['%.2f'%f] = edge_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    graphs = []\n",
    "\n",
    "    g = Graph()\n",
    "    graphs.append(g)\n",
    "    graphs[0].add_vertex(num_neurons)\n",
    "    e_weight = graphs[0].new_ep(\"double\")\n",
    "    e_layer = graphs[0].new_ep(\"int\")\n",
    "    n_id = graphs[0].new_vp(\"int\", vals = [i for i in range(num_neurons)])\n",
    "    graphs[0].add_edge_list(processed_matrices['%.2f'%f][0], eprops=[e_weight, e_layer])\n",
    "    graphs[0].edge_properties[\"edge_weight\"] = e_weight\n",
    "    graphs[0].edge_properties[\"edge_layer\"] = e_layer\n",
    "    \n",
    "    \n",
    "    G = graphs[0]\n",
    "\n",
    "    for l in range(1,layers):\n",
    "        g = Graph()\n",
    "        graphs.append(g)\n",
    "        graphs[l].add_vertex(num_neurons)\n",
    "        e_weight = graphs[l].new_ep(\"double\")\n",
    "        e_layer = graphs[l].new_ep(\"int\")\n",
    "        n_id = graphs[l].new_vp(\"int\", vals = [i for i in range(num_neurons)])\n",
    "        graphs[l].add_edge_list(processed_matrices['%.2f'%f][l], eprops=[e_weight, e_layer])\n",
    "        graphs[l].edge_properties[\"edge_weight\"] = e_weight\n",
    "        graphs[l].edge_properties[\"edge_layer\"] = e_layer\n",
    "        \n",
    "        G = graph_union(G, graphs[l], include = False, internal_props = True)\n",
    "        \n",
    "    states = []\n",
    "\n",
    "    state = LayeredBlockState(G, deg_corr = True, ec = G.ep.edge_layer,  recs=[G.ep.edge_weight], rec_types=[\"real-exponential\"],  layers = True, overlap = True)\n",
    "    \n",
    "    S1 = state.entropy()\n",
    "    states.append(state)\n",
    "    # Equilibrate\n",
    "    mcmc_equilibrate(state, force_niter=50, mcmc_args=dict(niter=10))\n",
    "\n",
    "    dls = []         # description length history\n",
    "    bs = []          # partitions\n",
    "    S2 = state.entropy()\n",
    "    def collect_partitions(s):\n",
    "        global bs, dls\n",
    "        bs.append(s.get_state())\n",
    "        dls.append(s.entropy())\n",
    "\n",
    "    # Now we collect 2000 partitions; but the larger this is, the\n",
    "    # more accurate will be the calculation\n",
    "    \n",
    "    states.append(state)\n",
    "    \n",
    "    mcmc_equilibrate(state, force_niter=50, mcmc_args=dict(niter=5), callback=collect_partitions)\n",
    "    \n",
    "    S3 = state.entropy()\n",
    "    states.append(state)\n",
    "    all_states['%.2f'%f] = states\n",
    "    print(S1,S2,S3, S2-S1, S3-S2, S3-S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memberships_0 = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    node_ids = []\n",
    "    for i,e in enumerate(all_states['%.2f'%f][0].get_nonoverlap_blocks()):\n",
    "        node_ids.append(e)\n",
    "    number_of_colors = len(np.unique(node_ids))\n",
    "\n",
    "    membership = [[] for i in range(number_of_colors)]\n",
    "    for i in range(num_neurons):#(num_neurons*layers):\n",
    "        for j in range(layers):\n",
    "            node_id = node_ids[j*num_neurons+i]\n",
    "            membership[node_id].append((i,j))\n",
    "    memberships_0['%.2f'%f] = membership\n",
    "\n",
    "memberships_1 = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    node_ids = []\n",
    "    for i,e in enumerate(all_states['%.2f'%f][1].get_nonoverlap_blocks()):\n",
    "        node_ids.append(e)\n",
    "    number_of_colors = len(np.unique(node_ids))\n",
    "\n",
    "    membership = [[] for i in range(number_of_colors)]\n",
    "    for i in range(num_neurons):#(num_neurons*layers):\n",
    "        for j in range(layers):\n",
    "            node_id = node_ids[j*num_neurons+i]\n",
    "            membership[node_id].append((i,j))\n",
    "    memberships_1['%.2f'%f] = membership\n",
    "\n",
    "memberships_2 = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    node_ids = []\n",
    "    for i,e in enumerate(all_states['%.2f'%f][2].get_nonoverlap_blocks()):\n",
    "        node_ids.append(e)\n",
    "    number_of_colors = len(np.unique(node_ids))\n",
    "\n",
    "    membership = [[] for i in range(number_of_colors)]\n",
    "    for i in range(num_neurons):#(num_neurons*layers):\n",
    "        for j in range(layers):\n",
    "            node_id = node_ids[j*num_neurons+i]\n",
    "            membership[node_id].append((i,j))\n",
    "    memberships_2['%.2f'%f] = membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_deg_corr + \"DSBM_memberships_deg_corr_0.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(memberships_0, fp)\n",
    "with open(path_deg_corr + \"DSBM_memberships_deg_corr_1.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(memberships_1, fp)\n",
    "with open(path_deg_corr + \"DSBM_memberships_deg_corr_2.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(memberships_2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(len(threshs),3,figsize = (30,10*len(thresholds)))\n",
    "\n",
    "for k,f in enumerate(threshs):\n",
    "    comms_0 = np.zeros((num_neurons,layers))\n",
    "    color_0 = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(memberships_0['%.2f'%f]))]\n",
    "    \n",
    "    comms_1 = np.zeros((num_neurons,layers))\n",
    "    color_1 = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(memberships_1['%.2f'%f]))]\n",
    "    \n",
    "    comms_2 = np.zeros((num_neurons,layers))\n",
    "    color_2 = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(memberships_2['%.2f'%f]))]\n",
    "    \n",
    "    for i, l in enumerate(memberships_0['%.2f'%f]):\n",
    "        for j,m in enumerate(l):\n",
    "            comms_0[m[0]][m[1]] = i\n",
    "            \n",
    "    for i, l in enumerate(memberships_1['%.2f'%f]):\n",
    "        for j,m in enumerate(l):\n",
    "            comms_1[m[0]][m[1]] = i\n",
    "    \n",
    "    for i, l in enumerate(memberships_2['%.2f'%f]):\n",
    "        for j,m in enumerate(l):\n",
    "            comms_2[m[0]][m[1]] = i\n",
    "            \n",
    "    cmap_0 = mpl.colors.ListedColormap(color_0)\n",
    "    cmap_1 = mpl.colors.ListedColormap(color_1)\n",
    "    cmap_2 = mpl.colors.ListedColormap(color_2)\n",
    "\n",
    "    ax[k][0].imshow(comms_0, interpolation = 'none', cmap = cmap_0, aspect = 'auto', origin = 'lower')\n",
    "    ax[k][1].imshow(comms_1, interpolation = 'none', cmap = cmap_1, aspect = 'auto', origin = 'lower')\n",
    "    ax[k][2].imshow(comms_2, interpolation = 'none', cmap = cmap_2, aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "    ax[k][0].set_xticks([i for i in range(layers)])\n",
    "    ax[k][0].set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "    ax[k][0].tick_params(axis = 'both', labelsize = 12)\n",
    "    ax[k][0].set_xlabel('Layers (Time)', fontsize = 25)\n",
    "    ax[k][0].set_ylabel('Neuron ID', fontsize = 25)\n",
    "    ax[k][0].set_title('%d Communities, threshold:%.3f'%(len(memberships_0['%.2f'%f]),threshs[k]), fontsize=29)\n",
    "    \n",
    "    ax[k][1].set_xticks([i for i in range(layers)])\n",
    "    ax[k][1].set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "    ax[k][1].tick_params(axis = 'both', labelsize = 12)\n",
    "    ax[k][1].set_xlabel('Layers (Time)', fontsize = 25)\n",
    "    ax[k][1].set_ylabel('Neuron ID', fontsize = 25)\n",
    "    ax[k][1].set_title('%d Communities, threshold:%.3f'%(len(memberships_1['%.2f'%f]),threshs[k]), fontsize=29)\n",
    "    \n",
    "    ax[k][2].set_xticks([i for i in range(layers)])\n",
    "    ax[k][2].set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "    ax[k][2].tick_params(axis = 'both', labelsize = 12)\n",
    "    ax[k][2].set_xlabel('Layers (Time)', fontsize = 25)\n",
    "    ax[k][2].set_ylabel('Neuron ID', fontsize = 25)\n",
    "    ax[k][2].set_title('%d Communities, threshold:%.3f'%(len(memberships_2['%.2f'%f]),threshs[k]), fontsize=29)\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_deg_corr + 'communities.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt",
   "language": "python",
   "name": "gt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
