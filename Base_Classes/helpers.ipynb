{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import random\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from math import floor\n",
    "\n",
    "from elephant.spike_train_generation import homogeneous_poisson_process\n",
    "import elephant.conversion as conv\n",
    "import neo as n\n",
    "import quantities as pq\n",
    "from quantities import Hz, s, ms\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_cross_corr(x,y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    x_cov_std = np.nanmax(np.sqrt(np.correlate(x - x_mean, x - x_mean, 'full')))\n",
    "    y_cov_std = np.nanmax(np.sqrt(np.correlate(y - y_mean, y - y_mean, 'full')))\n",
    "\n",
    "    normalization = x_cov_std * y_cov_std\n",
    "        \n",
    "\n",
    "    unnormalized_correlation = np.correlate(x - x_mean, y - y_mean, 'full')\n",
    "    \n",
    "    corr_array = unnormalized_correlation/normalization\n",
    "\n",
    "    return(corr_array)\n",
    "\n",
    "def max_norm_cross_corr(x1, x2):\n",
    "    \n",
    "    correlation= normalized_cross_corr(x1, x2)\n",
    "    \n",
    "    lag = abs(correlation).argmax() - len(x1)+1\n",
    "    \n",
    "    max_corr = max(abs(correlation))\n",
    "    \n",
    "    return(max_corr, lag)\n",
    "\n",
    "def cross_correlation_matrix(data):\n",
    "    #input: n x t matrix where n is the number of rois and t is the duration of the time series\n",
    "    #return: n x n symmetric cross correlation matrix, nxn uppertriangular cross correlation matrix and lag matrix\n",
    "    n, t = data.shape\n",
    "    X = np.zeros((n,n))\n",
    "    lag = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        for j in range(i+1,n):\n",
    "            X[i][j],lag[i][j] = max_norm_cross_corr(data[i,:],data[j,:])\n",
    "    X[np.isnan(X)] = 0\n",
    "    lag[np.isnan(lag)] = 0\n",
    "    \n",
    "    X_full = X + X.T\n",
    "    lag = lag + lag.T\n",
    "    return(X_full, X, lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_time_series(array, binsize, gaussian = True, **kwargs):\n",
    "        #input: nxt matrix \n",
    "        #returns: binned time series i.e. l x n x binsize\n",
    "        \n",
    "    n = array.shape[0] # number of neurons\n",
    "    totalsize = array.shape[1] # total duration of spikes\n",
    "    gauss_array = np.zeros((n,totalsize))\n",
    "    l = int(totalsize/binsize) # number of resulting layers\n",
    "        \n",
    "    if gaussian:\n",
    "        for i in range(n):\n",
    "            gauss_array[i] = gaussian_filter(array[i],kwargs['sigma'])\n",
    "    else: gauss_array = array\n",
    "            \n",
    "    A = np.zeros((l,n,binsize))\n",
    "    for i in range(l):\n",
    "        A[i] = gauss_array[:,i*binsize:(i+1)*binsize]\n",
    "    return(A)\n",
    "\n",
    "def binarize(array):\n",
    "    n,t = array.shape\n",
    "    binary_spikes = np.zeros((n,t))\n",
    "    for i in range(n):\n",
    "        for j in range(t):\n",
    "            if array[i][j] == 0: pass\n",
    "            else: binary_spikes[i][j] = 1\n",
    "    return(binary_spikes)\n",
    "\n",
    "def threshold(array, thresh):\n",
    "    n,t = array.shape\n",
    "    thresholded_array = np.copy(array)\n",
    "    for i in range(n):\n",
    "        for j in range(t):\n",
    "            if array[i][j] < thresh: thresholded_array[i][j] = 0\n",
    "            else: pass\n",
    "    return(thresholded_array)\n",
    "\n",
    "def gaussian_filter(array,sigma):\n",
    "    #sigma=0.25==gaussian kernel with length 3\n",
    "    #sigma=0.5==gaussian kernel with length 5\n",
    "    #sigma=1==gaussian kernel with length 9\n",
    "    return(gaussian_filter1d(array,sigma))\n",
    "\n",
    "def jitter(spike, k):\n",
    "    #jittering the given spike train\n",
    "    jittered = np.zeros(spike.shape)\n",
    "    for i in np.nonzero(spike)[1]:\n",
    "        jitt = random.randint(-k,k)\n",
    "        try:jittered[0,i+jitt] = 1\n",
    "        except:jittered[0,i] = 1\n",
    "    return(jittered)\n",
    "\n",
    "def spike_count(spikes, ax, num_bins = None, t_min = None, t_max = None):\n",
    "    n,t = spikes.shape\n",
    "    if t_min is None: t_min = 0\n",
    "    if t_max is None: t_max = t\n",
    "    if t_max<=t_min: raise ValueError('t_min should be less than t_max')\n",
    "    spike_count = []\n",
    "    binary = binarize(spikes)\n",
    "    for i in range(n):\n",
    "        spike_count.append(np.sum(binary[i][t_min:t_max]))\n",
    "    if num_bins is None: num_bins = int(np.max(spike_count) - np.min(spike_count))\n",
    "    n, bins, patches = ax.hist(spike_count, num_bins, color = 'blue')\n",
    "    ax.set_title(\"Spike Rate Distribution\")\n",
    "    ax.set_xlabel(\"Total Number of Spikes\", fontsize = 22)\n",
    "    ax.set_ylabel(\"Number of Neurons\", fontsize = 22)\n",
    "    return(n,bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ground_truth(comm_sizes, method = 'scattered', pad = False, community_operation = 'grow'):\n",
    "    ##genertaes community labels according to two methods one in which the rest of the network except the planted communities\n",
    "    # are scattered i.e. they all have their own community or they are all in one community, integrated.\n",
    "    if community_operation == 'grow':\n",
    "        layers = len(comm_sizes)\n",
    "        if method == 'scattered':\n",
    "            truth_labels = [0 for i in range(sum(comm_sizes[:1]))] + list(np.arange(1, sum(comm_sizes[1:])+1))\n",
    "            \n",
    "            truth_labels_tip = truth_labels\n",
    "                     \n",
    "            for j in range(2,layers):\n",
    "                truth_labels = truth_labels + [0 for i in range(sum(comm_sizes[:j]))] + truth_labels_tip[sum(comm_sizes[:j]):]\n",
    "            \n",
    "            truth_labels = truth_labels + [0 for i in range(sum(comm_sizes[:layers]))]\n",
    "            \n",
    "            if pad:\n",
    "                truth_labels = truth_labels_tip + truth_labels\n",
    "                truth_labels = truth_labels + [0 for i in range(sum(comm_sizes[:layers]))]  \n",
    "        \n",
    "        if method == 'integrated':\n",
    "        \n",
    "            truth_labels = [0 for i in range(sum(comm_sizes[:1]))] + [1 for i in range(sum(comm_sizes[1:]))]\n",
    "            if pad: truth_labels = truth_labels + truth_labels\n",
    "            for j in range(2,layers):\n",
    "                truth_labels = truth_labels + [0 for i in range(sum(comm_sizes[:j]))] + [1 for i in range(sum(comm_sizes[j:]))]\n",
    "\n",
    "            if pad:\n",
    "                truth_labels = truth_labels + [0 for i in range(sum(comm_sizes[:layers]))]\n",
    "                truth_labels = truth_labels + [0 for i in range(sum(comm_sizes[:layers]))]\n",
    "                \n",
    "    elif community_operation == 'merge': ##only for two layers\n",
    "        truth_labels = []\n",
    "        \n",
    "        for j,f in enumerate(comm_sizes[0]):\n",
    "            truth_labels = truth_labels + [j for k in range(f)]\n",
    "    \n",
    "        for j,f in enumerate([6,3,7]):##communities that are merged in the first layer are assigned one of the labels of \n",
    "            #merged communities \n",
    "            truth_labels = truth_labels + [f for i in range(comm_sizes[1][j])]\n",
    "            \n",
    "        if pad:\n",
    "            l1 = truth_labels[:sum(comm_sizes[0])]\n",
    "            l2 = truth_labels[sum(comm_sizes[0]):]\n",
    "            truth_labels = l1 + truth_labels +l2\n",
    "    return(truth_labels)\n",
    "\n",
    "def information_recovery(pred_labels, comm_size, truth, interlayers, other_parameter, com_op):\n",
    "    NMI1 = np.zeros((len(interlayers), len(other_parameter)))\n",
    "    ARI1 = np.zeros((len(interlayers), len(other_parameter)))\n",
    "    F1S1 = np.zeros((len(interlayers), len(other_parameter)))\n",
    "    \n",
    "    if truth == 'Scattered': true_labels = generate_ground_truth(comm_size, method = 'scattered', pad = True, community_operation = com_op)\n",
    "    if truth == 'Integrated': true_labels = generate_ground_truth(comm_size, method = 'integrated', pad = True, community_operation = com_op)\n",
    "\n",
    "    \n",
    "    for i in range(len(interlayers)):\n",
    "        for j in range(len(other_parameter)):\n",
    "            NMI1[i][j] = normalized_mutual_info_score(true_labels, list(pred_labels[i*len(other_parameter)+j].astype(int)), average_method = 'max')\n",
    "            ARI1[i][j] = adjusted_rand_score(true_labels, list(pred_labels[i*len(other_parameter)+j].astype(int)))\n",
    "            F1S1[i][j] = f1_score(true_labels, list(pred_labels[i*len(other_parameter)+j].astype(int)), average = 'weighted')\n",
    "        \n",
    "    fig,ax = plt.subplots(1,3, figsize = (50, 25))\n",
    "    c = ax[0].imshow(NMI1, origin = 'lower', \n",
    "                     interpolation = 'none', \n",
    "                     cmap = 'Reds', aspect = 'auto',\n",
    "                     extent = [other_parameter[0]-0.005, other_parameter[-1]+0.005, interlayers[0]-0.005, interlayers[-1]+0.005])\n",
    "\n",
    "    c = ax[1].imshow(ARI1, origin = 'lower', \n",
    "                     interpolation = 'none', \n",
    "                     cmap = 'Reds', aspect = 'auto',\n",
    "                     extent = [other_parameter[0]-0.005, other_parameter[-1]+0.005, interlayers[0]-0.005, interlayers[-1]+0.005])\n",
    "\n",
    "    c = ax[2].imshow(F1S1, origin = 'lower',\n",
    "                     interpolation = 'none', \n",
    "                     cmap = 'Reds', aspect = 'auto',\n",
    "                     extent = [other_parameter[0]-0.005, other_parameter[-1]+0.005, interlayers[0]-0.005, interlayers[-1]+0.005])\n",
    "\n",
    "    ax[0].set_title('NMI wrt %s Ground Truth'%truth, fontsize = 30)\n",
    "    ax[0].set_xlabel('Thresholds or Resolutions', fontsize = 25)\n",
    "    ax[0].set_ylabel('Interlayers', fontsize = 25)\n",
    "    ax[0].set_xticks(other_parameter)\n",
    "    ax[0].set_yticks(interlayers)\n",
    "    ax[0].tick_params(axis = 'both', labelsize = 15)\n",
    "\n",
    "    ax[1].set_title('ARI wrt %s Ground Truth'%truth, fontsize = 30)\n",
    "    ax[1].set_xlabel('Thresholds or Resolutions', fontsize = 25)\n",
    "    ax[1].set_ylabel('Interlayers', fontsize = 25)\n",
    "    ax[1].set_xticks(other_parameter)\n",
    "    ax[1].set_yticks(interlayers)\n",
    "    ax[1].tick_params(axis = 'both', labelsize = 15)\n",
    "\n",
    "    ax[2].set_title('F1-Score wrt %s Ground Truth'%truth, fontsize = 30)\n",
    "    ax[2].set_xlabel('Thresholds or Resolutions', fontsize = 25)\n",
    "    ax[2].set_ylabel('Interlayers', fontsize = 25)\n",
    "    ax[2].set_xticks(other_parameter)\n",
    "    ax[2].set_yticks(interlayers)\n",
    "    ax[2].tick_params(axis = 'both', labelsize = 15)\n",
    "    \n",
    "    cbar = fig.colorbar(c, ax = ax.flat, orientation = 'horizontal')\n",
    "    cbar.ax.tick_params(labelsize = 20) \n",
    "    \n",
    "def display_truth(comm_sizes, community_operation):\n",
    "    if community_operation == 'grow':\n",
    "        n = sum(comm_sizes)\n",
    "        layers = len(comm_sizes)\n",
    "        l = layers + 2\n",
    "    \n",
    "        scattered_truth = generate_ground_truth(comm_sizes, \n",
    "                                                method = 'scattered', \n",
    "                                                pad = True, \n",
    "                                                community_operation = community_operation)\n",
    "        number_of_colors = max(scattered_truth)+1\n",
    "    \n",
    "        membership = [[] for i in range(number_of_colors)]\n",
    "        for i,m in enumerate(scattered_truth):\n",
    "            time = floor(i/n)\n",
    "            node_id = i%n\n",
    "            membership[m].append((node_id,time))\n",
    "\n",
    "        fig,ax = plt.subplots(1,2, figsize = (16,8))\n",
    "\n",
    "        comms = np.zeros((n,layers+2))\n",
    "\n",
    "        color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(number_of_colors)]\n",
    "\n",
    "        for i, l in enumerate(membership):\n",
    "            for j,k in enumerate(l):\n",
    "                comms[k[0]][k[1]] = i\n",
    "\n",
    "        cmap = mpl.colors.ListedColormap(color)\n",
    "\n",
    "        ax[0].imshow(comms, interpolation = 'none', cmap = cmap, aspect = 'auto', origin = 'lower', extent = [-0.5,layers+2-0.5,-0.5,n-0.5])\n",
    "        ax[0].set_xticks([i for i in range(layers+2)])\n",
    "        ax[0].set_yticks([i*10 for i in range(int(n/10)+1)])\n",
    "        ax[0].tick_params(axis = 'both', labelsize = 15)\n",
    "        ax[0].set_xlabel('Layers (Time)', fontsize = 18)\n",
    "        ax[0].set_ylabel('Neuron ID', fontsize = 18)\n",
    "        ax[0].set_title('Scattered Ground Truth with %d Communities' %len(color), fontsize = 20)\n",
    "    \n",
    "        integrated_truth = generate_ground_truth(comm_sizes, \n",
    "                                                 method = 'integrated', \n",
    "                                                 pad = True, \n",
    "                                                 community_operation = community_operation)\n",
    "        number_of_colors = max(integrated_truth)+1\n",
    "        membership = [[] for i in range(number_of_colors)]\n",
    "        for i,m in enumerate(integrated_truth):\n",
    "            time = floor(i/n)\n",
    "            node_id = i%n\n",
    "            membership[m].append((node_id,time))\n",
    "\n",
    "        comms = np.zeros((n,layers+2))\n",
    "\n",
    "        color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(number_of_colors)]\n",
    "\n",
    "        for i, l in enumerate(membership):\n",
    "            for j,k in enumerate(l):\n",
    "                comms[k[0]][k[1]] = i\n",
    "\n",
    "        cmap = mpl.colors.ListedColormap(color)\n",
    "\n",
    "        ax[1].imshow(comms, interpolation = 'none', cmap = cmap, aspect = 'auto', origin = 'lower', extent = [-0.5,layers+2-0.5,-0.5,n-0.5])\n",
    "        ax[1].set_xticks([i for i in range(layers+2)])\n",
    "        ax[1].set_yticks([i*10 for i in range(int(n/10)+1)])\n",
    "        ax[1].tick_params(axis = 'both', labelsize = 15)\n",
    "        ax[1].set_xlabel('Layers (Time)', fontsize = 18)\n",
    "        ax[1].set_ylabel('Neuron ID', fontsize = 18)\n",
    "        ax[1].set_title('Integrated Ground Truth with %d Communities' %len(color), fontsize = 20)\n",
    "    \n",
    "    elif community_operation == 'merge':\n",
    "        n = sum(comm_sizes[0])\n",
    "        layers = len(comm_sizes)\n",
    "        l = layers + 2\n",
    "        \n",
    "        truth = generate_ground_truth(comm_sizes, pad = True, community_operation = 'merge')\n",
    "        \n",
    "        number_of_colors = max(truth)+1\n",
    "    \n",
    "        membership = [[] for i in range(number_of_colors)]\n",
    "        for i,m in enumerate(truth):\n",
    "            time = floor(i/n)\n",
    "            node_id = i%n\n",
    "            membership[m].append((node_id,time))\n",
    "\n",
    "        fig,ax = plt.subplots(1,1, figsize = (8,8))\n",
    "\n",
    "        comms = np.zeros((n,layers+2))\n",
    "\n",
    "        color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(number_of_colors)]\n",
    "\n",
    "        for i, l in enumerate(membership):\n",
    "            for j,k in enumerate(l):\n",
    "                comms[k[0]][k[1]] = i\n",
    "\n",
    "        cmap = mpl.colors.ListedColormap(color)\n",
    "\n",
    "        ax.imshow(comms, interpolation = 'none', cmap = cmap, aspect = 'auto', origin = 'lower', extent = [-0.5,layers+2-0.5,-0.5,n-0.5])\n",
    "        ax.set_xticks([i for i in range(layers+2)])\n",
    "        ax.set_yticks([i*10 for i in range(int(n/10)+1)])\n",
    "        ax.tick_params(axis = 'both', labelsize = 15)\n",
    "        ax.set_xlabel('Layers (Time)', fontsize = 18)\n",
    "        ax.set_ylabel('Neuron ID', fontsize = 18)\n",
    "        ax.set_title('Ground Truth with %d Communities' %len(color), fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_series(operation, community_sizes, spiking_rates, spy = True, windowsize = 1000, k = 5):\n",
    "    \n",
    "    binsize = windowsize\n",
    "    layers = len(community_sizes)\n",
    "    total_duration = int(layers*binsize)\n",
    "    \n",
    "    if operation == 'grow':    \n",
    "        num_neurons = int(sum(community_sizes))\n",
    "        spikes = np.zeros((num_neurons,total_duration))\n",
    "        master_spike = np.zeros((1,total_duration))\n",
    "        master = homogeneous_poisson_process(rate = spiking_rates[0]*Hz, \n",
    "                                             t_start = 0.0*ms, \n",
    "                                             t_stop = total_duration*ms, \n",
    "                                             as_array = True) \n",
    "        for i,e in enumerate(master):    \n",
    "            master_spike[0][int(e)] = 1\n",
    "        for i in range(community_sizes[0]):\n",
    "            spikes[i] = jitter(master_spike, k)\n",
    "        \n",
    "        comms = []\n",
    "        for i in range(1,layers):\n",
    "            comms.append([homogeneous_poisson_process(rate = spiking_rates[i]*Hz, \n",
    "                                                      t_start = 0.0*ms, \n",
    "                                                      t_stop = i*binsize*ms, \n",
    "                                                      as_array = True) for j in range(community_sizes[i])])\n",
    "        neuron_count = community_sizes[0]\n",
    "        for i,e in enumerate(comms):\n",
    "            for j,f in enumerate(e):\n",
    "                for k,m in enumerate(f):\n",
    "                    spikes[neuron_count+j][int(m)] = 1\n",
    "            neuron_count = neuron_count + len(e)\n",
    "        \n",
    "        neuron_count = community_sizes[0]\n",
    "        for i in range(1, len(comms)+1):\n",
    "            for j in range(neuron_count, neuron_count + community_sizes[i]):\n",
    "                for k in np.nonzero(spikes[0][(i*binsize):])[0]:\n",
    "                    jitt = random.randint(-5,5)\n",
    "                    try:spikes[j,(i*binsize)+k+jitt] = 1\n",
    "                    except:spikes[j,k] = 1\n",
    "            neuron_count = neuron_count + community_sizes[i]\n",
    "            \n",
    "    if operation == 'merge':\n",
    "        num_neurons = int(sum(community_sizes[0]))\n",
    "        \n",
    "        spikes = np.zeros((num_neurons,total_duration))\n",
    "        \n",
    "        for s in range(layers):\n",
    "            neuron_count = 0\n",
    "            for i,e in enumerate(community_sizes[s]):\n",
    "                initial_master = homogeneous_poisson_process(rate = spiking_rates[s][i]*Hz,\n",
    "                                                             t_start = s*(binsize)*ms, \n",
    "                                                             t_stop = (s+1)*binsize*ms, \n",
    "                                                             as_array = True)\n",
    "                master_spikes = np.zeros((1,total_duration))\n",
    "    \n",
    "                for j,f in enumerate(initial_master):\n",
    "                    master_spikes[0][int(f)] = 1\n",
    "\n",
    "                for j in range(e):\n",
    "                    spikes[neuron_count+j][int(s*binsize):int((s+1)*binsize)] = jitter(\n",
    "                        master_spikes[:,int(s*binsize):int((s+1)*binsize)], k)\n",
    "                neuron_count = neuron_count + e\n",
    "        \n",
    "            \n",
    "    if spy:\n",
    "        fig,ax = plt.subplots(1,1,figsize=(20,10))\n",
    "        ax.imshow(spikes, origin = 'lower', interpolation='nearest', aspect='auto', \n",
    "                  extent = [0,total_duration,0,num_neurons])\n",
    "        ax.set_title('Spike Trains generated via Poisson Process for %d synthetic neurons'%num_neurons, \n",
    "                     fontsize = 30)\n",
    "        ax.set_xlabel('TIME (in Miliseconds)', fontsize = 20)\n",
    "        ax.set_xticks([j*binsize for j in range(int(total_duration/binsize)+1)])\n",
    "        ax.set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "        ax.set_ylabel('Neuron ID', fontsize = 25)\n",
    "        ax.set_xlabel('Time (Frames)', fontsize = 20)\n",
    "        ax.tick_params(axis = 'both', labelsize = 20)\n",
    "            \n",
    "    return(spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
