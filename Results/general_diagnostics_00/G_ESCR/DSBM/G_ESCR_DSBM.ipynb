{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from graph_tool.all import *\n",
    "\n",
    "base_path = \"/projects/academic/smuldoon/bengieru/Community_Detection/general_diagnostics_00/\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, base_path)\n",
    "\n",
    "from helpers import *\n",
    "from Temporal_Community_Detection import temporal_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = base_path + 'G_ESCR/'\n",
    "\n",
    "path = data_path + 'DSBM/'\n",
    "\n",
    "with open(data_path + 'spikes.pkl', 'rb') as handle:\n",
    "    spikes = pickle.load(handle)\n",
    "    \n",
    "with open(data_path + 'comm_size.pkl', 'rb') as handle:\n",
    "    comm_sizes = pickle.load(handle)\n",
    "    \n",
    "num_neurons = sum(comm_sizes)\n",
    "layers = 7\n",
    "\n",
    "window_size = 1000 # size, in frames, each adjacency matrix correspond to. better to be equal to bin_size \n",
    "standard_dev = 1.2 # for gaussian kernel\n",
    "k = 5 #for jittering the spikes\n",
    "pad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_spikes = bin_time_series(spikes, window_size, gaussian = True, sigma = standard_dev)\n",
    "\n",
    "adjacency_matrices = []\n",
    "for i in range(layers):\n",
    "    adjacency_matrices.append(cross_correlation_matrix(binned_spikes[i])[0])\n",
    "    \n",
    "if pad:\n",
    "    padded_adjacencies = [adjacency_matrices[0]]  + adjacency_matrices + [adjacency_matrices[-1]]\n",
    "    layers = layers + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = temporal_network(num_neurons, \n",
    "                      layers, \n",
    "                      window_size, \n",
    "                      data = 'list__adjacency', \n",
    "                      list_adjacency = padded_adjacencies, \n",
    "                      omega = 1, \n",
    "                      kind = 'ordinal')\n",
    "\n",
    "fig,ax = plt.subplots(1,1, figsize = (25,15))\n",
    "TN.raster_plot(spikes, ax)\n",
    "plt.savefig(path + 'raster_plot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = 26\n",
    "threshs = np.linspace(0, 0.5, grid)\n",
    "\n",
    "path_deg_corr = path + 'deg_corr/'\n",
    "\n",
    "os.makedirs(path_deg_corr, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_matrices = {}\n",
    "for k, f in enumerate(threshs):\n",
    "    edge_lists = [[] for i in range(layers)]\n",
    "    for i in range(layers):\n",
    "        A = padded_adjacencies[i]\n",
    "        firing = np.transpose(np.nonzero(A))\n",
    "        for j,m in enumerate(firing):\n",
    "            if A[m[0],m[1]]<f: pass\n",
    "            else: \n",
    "                quadreplet =(m[0], m[1], A[m[0], m[1]], i)\n",
    "                edge_lists[i].append(quadreplet)\n",
    "    processed_matrices['%.2f'%f] = edge_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    graphs = []\n",
    "\n",
    "    g = Graph()\n",
    "    graphs.append(g)\n",
    "    graphs[0].add_vertex(num_neurons)\n",
    "    e_weight = graphs[0].new_ep(\"double\")\n",
    "    e_layer = graphs[0].new_ep(\"int\")\n",
    "    n_id = graphs[0].new_vp(\"int\", vals = [i for i in range(num_neurons)])\n",
    "    graphs[0].add_edge_list(processed_matrices['%.2f'%f][0], eprops=[e_weight, e_layer])\n",
    "    graphs[0].edge_properties[\"edge_weight\"] = e_weight\n",
    "    graphs[0].edge_properties[\"edge_layer\"] = e_layer\n",
    "    \n",
    "    \n",
    "    G = graphs[0]\n",
    "\n",
    "    for l in range(1,layers):\n",
    "        g = Graph()\n",
    "        graphs.append(g)\n",
    "        graphs[l].add_vertex(num_neurons)\n",
    "        e_weight = graphs[l].new_ep(\"double\")\n",
    "        e_layer = graphs[l].new_ep(\"int\")\n",
    "        n_id = graphs[l].new_vp(\"int\", vals = [i for i in range(num_neurons)])\n",
    "        graphs[l].add_edge_list(processed_matrices['%.2f'%f][l], eprops=[e_weight, e_layer])\n",
    "        graphs[l].edge_properties[\"edge_weight\"] = e_weight\n",
    "        graphs[l].edge_properties[\"edge_layer\"] = e_layer\n",
    "        \n",
    "        G = graph_union(G, graphs[l], include = False, internal_props = True)\n",
    "        \n",
    "    states = []\n",
    "\n",
    "    state = LayeredBlockState(G, deg_corr = True, ec = G.ep.edge_layer,  recs=[G.ep.edge_weight], rec_types=[\"real-exponential\"],  layers = True, overlap = True)\n",
    "    \n",
    "    S1 = state.entropy()\n",
    "    states.append(state)\n",
    "    # Equilibrate\n",
    "    mcmc_equilibrate(state, force_niter=50, mcmc_args=dict(niter=10))\n",
    "\n",
    "    dls = []         # description length history\n",
    "    bs = []          # partitions\n",
    "    S2 = state.entropy()\n",
    "    def collect_partitions(s):\n",
    "        global bs, dls\n",
    "        bs.append(s.get_state())\n",
    "        dls.append(s.entropy())\n",
    "\n",
    "    # Now we collect 2000 partitions; but the larger this is, the\n",
    "    # more accurate will be the calculation\n",
    "    \n",
    "    states.append(state)\n",
    "    \n",
    "    mcmc_equilibrate(state, force_niter=50, mcmc_args=dict(niter=5), callback=collect_partitions)\n",
    "    \n",
    "    S3 = state.entropy()\n",
    "    states.append(state)\n",
    "    all_states['%.2f'%f] = states\n",
    "    print(S1,S2,S3, S2-S1, S3-S2, S3-S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memberships_0 = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    node_ids = []\n",
    "    for i,e in enumerate(all_states['%.2f'%f][0].get_nonoverlap_blocks()):\n",
    "        node_ids.append(e)\n",
    "    number_of_colors = len(np.unique(node_ids))\n",
    "\n",
    "    membership = [[] for i in range(number_of_colors)]\n",
    "    for i in range(num_neurons):#(num_neurons*layers):\n",
    "        for j in range(layers):\n",
    "            node_id = node_ids[j*num_neurons+i]\n",
    "            membership[node_id].append((i,j))\n",
    "    memberships_0['%.2f'%f] = membership\n",
    "\n",
    "memberships_1 = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    node_ids = []\n",
    "    for i,e in enumerate(all_states['%.2f'%f][1].get_nonoverlap_blocks()):\n",
    "        node_ids.append(e)\n",
    "    number_of_colors = len(np.unique(node_ids))\n",
    "\n",
    "    membership = [[] for i in range(number_of_colors)]\n",
    "    for i in range(num_neurons):#(num_neurons*layers):\n",
    "        for j in range(layers):\n",
    "            node_id = node_ids[j*num_neurons+i]\n",
    "            membership[node_id].append((i,j))\n",
    "    memberships_1['%.2f'%f] = membership\n",
    "\n",
    "memberships_2 = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    node_ids = []\n",
    "    for i,e in enumerate(all_states['%.2f'%f][2].get_nonoverlap_blocks()):\n",
    "        node_ids.append(e)\n",
    "    number_of_colors = len(np.unique(node_ids))\n",
    "\n",
    "    membership = [[] for i in range(number_of_colors)]\n",
    "    for i in range(num_neurons):#(num_neurons*layers):\n",
    "        for j in range(layers):\n",
    "            node_id = node_ids[j*num_neurons+i]\n",
    "            membership[node_id].append((i,j))\n",
    "    memberships_2['%.2f'%f] = membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_deg_corr + \"DSBM_memberships_deg_corr_0.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(memberships_0, fp)\n",
    "with open(path_deg_corr + \"DSBM_memberships_deg_corr_1.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(memberships_1, fp)\n",
    "with open(path_deg_corr + \"DSBM_memberships_deg_corr_2.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(memberships_2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(len(threshs),3,figsize = (30,10*len(thresholds)))\n",
    "\n",
    "for k,f in enumerate(threshs):\n",
    "    comms_0 = np.zeros((num_neurons,layers))\n",
    "    color_0 = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(memberships_0['%.2f'%f]))]\n",
    "    \n",
    "    comms_1 = np.zeros((num_neurons,layers))\n",
    "    color_1 = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(memberships_1['%.2f'%f]))]\n",
    "    \n",
    "    comms_2 = np.zeros((num_neurons,layers))\n",
    "    color_2 = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(memberships_2['%.2f'%f]))]\n",
    "    \n",
    "    for i, l in enumerate(memberships_0['%.2f'%f]):\n",
    "        for j,m in enumerate(l):\n",
    "            comms_0[m[0]][m[1]] = i\n",
    "            \n",
    "    for i, l in enumerate(memberships_1['%.2f'%f]):\n",
    "        for j,m in enumerate(l):\n",
    "            comms_1[m[0]][m[1]] = i\n",
    "    \n",
    "    for i, l in enumerate(memberships_2['%.2f'%f]):\n",
    "        for j,m in enumerate(l):\n",
    "            comms_2[m[0]][m[1]] = i\n",
    "            \n",
    "    cmap_0 = mpl.colors.ListedColormap(color_0)\n",
    "    cmap_1 = mpl.colors.ListedColormap(color_1)\n",
    "    cmap_2 = mpl.colors.ListedColormap(color_2)\n",
    "\n",
    "    ax[k][0].imshow(comms_0, interpolation = 'none', cmap = cmap_0, aspect = 'auto', origin = 'lower')\n",
    "    ax[k][1].imshow(comms_1, interpolation = 'none', cmap = cmap_1, aspect = 'auto', origin = 'lower')\n",
    "    ax[k][2].imshow(comms_2, interpolation = 'none', cmap = cmap_2, aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "    ax[k][0].set_xticks([i for i in range(layers)])\n",
    "    ax[k][0].set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "    ax[k][0].tick_params(axis = 'both', labelsize = 12)\n",
    "    ax[k][0].set_xlabel('Layers (Time)', fontsize = 25)\n",
    "    ax[k][0].set_ylabel('Neuron ID', fontsize = 25)\n",
    "    ax[k][0].set_title('%d Communities, threshold:%.3f'%(len(memberships_0['%.2f'%f]),threshs[k]), fontsize=29)\n",
    "    \n",
    "    ax[k][1].set_xticks([i for i in range(layers)])\n",
    "    ax[k][1].set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "    ax[k][1].tick_params(axis = 'both', labelsize = 12)\n",
    "    ax[k][1].set_xlabel('Layers (Time)', fontsize = 25)\n",
    "    ax[k][1].set_ylabel('Neuron ID', fontsize = 25)\n",
    "    ax[k][1].set_title('%d Communities, threshold:%.3f'%(len(memberships_1['%.2f'%f]),threshs[k]), fontsize=29)\n",
    "    \n",
    "    ax[k][2].set_xticks([i for i in range(layers)])\n",
    "    ax[k][2].set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "    ax[k][2].tick_params(axis = 'both', labelsize = 12)\n",
    "    ax[k][2].set_xlabel('Layers (Time)', fontsize = 25)\n",
    "    ax[k][2].set_ylabel('Neuron ID', fontsize = 25)\n",
    "    ax[k][2].set_title('%d Communities, threshold:%.3f'%(len(memberships_2['%.2f'%f]),threshs[k]), fontsize=29)\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_deg_corr + 'communities.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = 26\n",
    "threshs = np.linspace(0, 0.5, grid)\n",
    "\n",
    "path_non_deg_corr = path + 'non_deg_corr/'\n",
    "\n",
    "os.makedirs(path_non_deg_corr, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_states = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    graphs = []\n",
    "\n",
    "    g = Graph()\n",
    "    graphs.append(g)\n",
    "    graphs[0].add_vertex(num_neurons)\n",
    "    e_weight = graphs[0].new_ep(\"double\")\n",
    "    e_layer = graphs[0].new_ep(\"int\")\n",
    "    n_id = graphs[0].new_vp(\"int\", vals = [i for i in range(num_neurons)])\n",
    "    graphs[0].add_edge_list(processed_matrices['%.2f'%f][0], eprops=[e_weight, e_layer])\n",
    "    graphs[0].edge_properties[\"edge_weight\"] = e_weight\n",
    "    graphs[0].edge_properties[\"edge_layer\"] = e_layer\n",
    "    \n",
    "    \n",
    "    G = graphs[0]\n",
    "\n",
    "    for l in range(1,layers):\n",
    "        g = Graph()\n",
    "        graphs.append(g)\n",
    "        graphs[l].add_vertex(num_neurons)\n",
    "        e_weight = graphs[l].new_ep(\"double\")\n",
    "        e_layer = graphs[l].new_ep(\"int\")\n",
    "        n_id = graphs[l].new_vp(\"int\", vals = [i for i in range(num_neurons)])\n",
    "        graphs[l].add_edge_list(processed_matrices['%.2f'%f][l], eprops=[e_weight, e_layer])\n",
    "        graphs[l].edge_properties[\"edge_weight\"] = e_weight\n",
    "        graphs[l].edge_properties[\"edge_layer\"] = e_layer\n",
    "        \n",
    "        G = graph_union(G, graphs[l], include = False, internal_props = True)\n",
    "        \n",
    "    states = []\n",
    "\n",
    "    state = LayeredBlockState(G, deg_corr = False, ec = G.ep.edge_layer,  recs=[G.ep.edge_weight], rec_types=[\"real-exponential\"],  layers = True, overlap = True)\n",
    "    \n",
    "    S1 = state.entropy()\n",
    "    states.append(state)\n",
    "    # Equilibrate\n",
    "    mcmc_equilibrate(state, force_niter=50, mcmc_args=dict(niter=10))\n",
    "\n",
    "    dls = []         # description length history\n",
    "    bs = []          # partitions\n",
    "    S2 = state.entropy()\n",
    "    def collect_partitions(s):\n",
    "        global bs, dls\n",
    "        bs.append(s.get_state())\n",
    "        dls.append(s.entropy())\n",
    "\n",
    "    # Now we collect 2000 partitions; but the larger this is, the\n",
    "    # more accurate will be the calculation\n",
    "    \n",
    "    states.append(state)\n",
    "    \n",
    "    mcmc_equilibrate(state, force_niter=50, mcmc_args=dict(niter=5), callback=collect_partitions)\n",
    "    \n",
    "    S3 = state.entropy()\n",
    "    states.append(state)\n",
    "    all_states['%.2f'%f] = states\n",
    "    print(S1,S2,S3, S2-S1, S3-S2, S3-S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memberships_0 = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    node_ids = []\n",
    "    for i,e in enumerate(all_states['%.2f'%f][0].get_nonoverlap_blocks()):\n",
    "        node_ids.append(e)\n",
    "    number_of_colors = len(np.unique(node_ids))\n",
    "\n",
    "    membership = [[] for i in range(number_of_colors)]\n",
    "    for i in range(num_neurons):#(num_neurons*layers):\n",
    "        for j in range(layers):\n",
    "            node_id = node_ids[j*num_neurons+i]\n",
    "            membership[node_id].append((i,j))\n",
    "    memberships_0['%.2f'%f] = membership\n",
    "\n",
    "memberships_1 = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    node_ids = []\n",
    "    for i,e in enumerate(all_states['%.2f'%f][1].get_nonoverlap_blocks()):\n",
    "        node_ids.append(e)\n",
    "    number_of_colors = len(np.unique(node_ids))\n",
    "\n",
    "    membership = [[] for i in range(number_of_colors)]\n",
    "    for i in range(num_neurons):#(num_neurons*layers):\n",
    "        for j in range(layers):\n",
    "            node_id = node_ids[j*num_neurons+i]\n",
    "            membership[node_id].append((i,j))\n",
    "    memberships_1['%.2f'%f] = membership\n",
    "\n",
    "memberships_2 = {}\n",
    "for k,f in enumerate(threshs):\n",
    "    node_ids = []\n",
    "    for i,e in enumerate(all_states['%.2f'%f][2].get_nonoverlap_blocks()):\n",
    "        node_ids.append(e)\n",
    "    number_of_colors = len(np.unique(node_ids))\n",
    "\n",
    "    membership = [[] for i in range(number_of_colors)]\n",
    "    for i in range(num_neurons):#(num_neurons*layers):\n",
    "        for j in range(layers):\n",
    "            node_id = node_ids[j*num_neurons+i]\n",
    "            membership[node_id].append((i,j))\n",
    "    memberships_2['%.2f'%f] = membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_non_deg_corr + \"DSBM_memberships_non_deg_corr_0.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(memberships_0, fp)\n",
    "with open(path_non_deg_corr + \"DSBM_memberships_non_deg_corr_1.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(memberships_1, fp)\n",
    "with open(path_non_deg_corr + \"DSBM_memberships_non_deg_corr_2.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(memberships_2, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(len(threshs),3,figsize = (30,10*len(thresholds)))\n",
    "\n",
    "for k,f in enumerate(threshs):\n",
    "    comms_0 = np.zeros((num_neurons,layers))\n",
    "    color_0 = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(memberships_0['%.2f'%f]))]\n",
    "    \n",
    "    comms_1 = np.zeros((num_neurons,layers))\n",
    "    color_1 = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(memberships_1['%.2f'%f]))]\n",
    "    \n",
    "    comms_2 = np.zeros((num_neurons,layers))\n",
    "    color_2 = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(len(memberships_2['%.2f'%f]))]\n",
    "    \n",
    "    for i, l in enumerate(memberships_0['%.2f'%f]):\n",
    "        for j,m in enumerate(l):\n",
    "            comms_0[m[0]][m[1]] = i\n",
    "            \n",
    "    for i, l in enumerate(memberships_1['%.2f'%f]):\n",
    "        for j,m in enumerate(l):\n",
    "            comms_1[m[0]][m[1]] = i\n",
    "    \n",
    "    for i, l in enumerate(memberships_2['%.2f'%f]):\n",
    "        for j,m in enumerate(l):\n",
    "            comms_2[m[0]][m[1]] = i\n",
    "            \n",
    "    cmap_0 = mpl.colors.ListedColormap(color_0)\n",
    "    cmap_1 = mpl.colors.ListedColormap(color_1)\n",
    "    cmap_2 = mpl.colors.ListedColormap(color_2)\n",
    "\n",
    "    ax[k][0].imshow(comms_0, interpolation = 'none', cmap = cmap_0, aspect = 'auto', origin = 'lower')\n",
    "    ax[k][1].imshow(comms_1, interpolation = 'none', cmap = cmap_1, aspect = 'auto', origin = 'lower')\n",
    "    ax[k][2].imshow(comms_2, interpolation = 'none', cmap = cmap_2, aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "    ax[k][0].set_xticks([i for i in range(layers)])\n",
    "    ax[k][0].set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "    ax[k][0].tick_params(axis = 'both', labelsize = 12)\n",
    "    ax[k][0].set_xlabel('Layers (Time)', fontsize = 25)\n",
    "    ax[k][0].set_ylabel('Neuron ID', fontsize = 25)\n",
    "    ax[k][0].set_title('%d Communities, threshold:%.3f'%(len(memberships_0['%.2f'%f]),threshs[k]), fontsize=29)\n",
    "    \n",
    "    ax[k][1].set_xticks([i for i in range(layers)])\n",
    "    ax[k][1].set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "    ax[k][1].tick_params(axis = 'both', labelsize = 12)\n",
    "    ax[k][1].set_xlabel('Layers (Time)', fontsize = 25)\n",
    "    ax[k][1].set_ylabel('Neuron ID', fontsize = 25)\n",
    "    ax[k][1].set_title('%d Communities, threshold:%.3f'%(len(memberships_1['%.2f'%f]),threshs[k]), fontsize=29)\n",
    "    \n",
    "    ax[k][2].set_xticks([i for i in range(layers)])\n",
    "    ax[k][2].set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "    ax[k][2].tick_params(axis = 'both', labelsize = 12)\n",
    "    ax[k][2].set_xlabel('Layers (Time)', fontsize = 25)\n",
    "    ax[k][2].set_ylabel('Neuron ID', fontsize = 25)\n",
    "    ax[k][2].set_title('%d Communities, threshold:%.3f'%(len(memberships_2['%.2f'%f]),threshs[k]), fontsize=29)\n",
    "plt.tight_layout()\n",
    "plt.savefig(path_non_deg_corr + 'communities.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
