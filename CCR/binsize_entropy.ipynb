{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import random\n",
    "from math import log, e\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.stats import zscore\n",
    "from math import floor\n",
    "import leidenalg as la\n",
    "import igraph as ig\n",
    "from elephant.spike_train_generation import homogeneous_poisson_process\n",
    "import elephant.conversion as conv\n",
    "import neo as n\n",
    "import quantities as pq\n",
    "from quantities import Hz, s, ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_cross_corr(x,y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    x_cov_std = np.nanmax(np.sqrt(np.correlate(x - x_mean, x - x_mean, 'full')))\n",
    "    y_cov_std = np.nanmax(np.sqrt(np.correlate(y - y_mean, y - y_mean, 'full')))\n",
    "\n",
    "    normalization = x_cov_std * y_cov_std\n",
    "        \n",
    "\n",
    "    unnormalized_correlation = np.correlate(x - x_mean, y - y_mean, 'full')\n",
    "    \n",
    "    corr_array = unnormalized_correlation/normalization\n",
    "\n",
    "    return(corr_array)\n",
    "\n",
    "def max_norm_cross_corr(x1, x2):\n",
    "    \n",
    "    correlation = normalized_cross_corr(x1, x2)\n",
    "    \n",
    "    lag = abs(correlation).argmax() - len(x1)+1\n",
    "    \n",
    "    max_corr = max(abs(correlation))\n",
    "    \n",
    "    return(max_corr, lag)\n",
    "\n",
    "def cross_correlation_matrix(data):\n",
    "    #input: n x t matrix where n is the number of rois and t is the duration of the time series\n",
    "    #return: n x n symmetric cross correlation matrix, nxn uppertriangular cross correlation matrix and lag matrix\n",
    "    n, t = data.shape\n",
    "    X = np.zeros((n,n))\n",
    "    lag = np.zeros((n,n))\n",
    "    \n",
    "    for i in range(n-1):\n",
    "        for j in range(i+1,n):\n",
    "            X[i][j],lag[i][j] = max_norm_cross_corr(data[i,:],data[j,:])\n",
    "    X[np.isnan(X)] = 0\n",
    "    lag[np.isnan(lag)] = 0\n",
    "    \n",
    "    X_full = X + X.T\n",
    "    lag = lag + lag.T\n",
    "    return(X_full, X, lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_time_series(array, binsize, gaussian = True, **kwargs):\n",
    "        #input: nxt matrix \n",
    "        #returns: binned time series i.e. l x n x binsize\n",
    "        \n",
    "    n = array.shape[0] # number of neurons\n",
    "    totalsize = array.shape[1] # total duration of spikes\n",
    "    gauss_array = np.zeros((n,totalsize))\n",
    "    l = int(totalsize/binsize) # number of resulting layers\n",
    "        \n",
    "    if gaussian:\n",
    "        for i in range(n):\n",
    "            gauss_array[i] = gaussian_filter(array[i],kwargs['sigma'])\n",
    "    else: gauss_array= array\n",
    "            \n",
    "    A = np.zeros((l,n,binsize))\n",
    "    for i in range(l):\n",
    "        A[i] = gauss_array[:,i*binsize:(i+1)*binsize]\n",
    "    return(A)\n",
    "\n",
    "def binarize(array):\n",
    "    n,t = array.shape\n",
    "    binary_spikes = np.zeros((n,t))\n",
    "    for i in range(n):\n",
    "        for j in range(t):\n",
    "            if array[i][j] == 0: pass\n",
    "            else: binary_spikes[i][j] = 1\n",
    "    return(binary_spikes)\n",
    "\n",
    "def gaussian_filter(array,sigma):\n",
    "    #sigma=0.25==gaussian kernel with length 3\n",
    "    #sigma=0.5==gaussian kernel with length 5\n",
    "    #sigma=1==gaussian kernel with length 9\n",
    "    return(gaussian_filter1d(array,sigma))\n",
    "\n",
    "def jitter(spike, k):\n",
    "    #jittering the given spike train\n",
    "    jittered = np.zeros(spike.shape)\n",
    "    for i in np.nonzero(spike)[1]:\n",
    "        jitt = random.randint(-k,k)\n",
    "        try:jittered[0,i+jitt] = 1\n",
    "        except:jittered[0,i] = 1\n",
    "    return(jittered)\n",
    "\n",
    "def spike_count(spikes, ax, num_bins = None, t_min = None, t_max = None):\n",
    "    n,t = spikes.shape\n",
    "    if t_min is None: t_min = 0\n",
    "    if t_max is None: t_max = t\n",
    "    if t_max<=t_min: raise ValueError('t_min should be less than t_max')\n",
    "    spike_count = []\n",
    "    binary = binarize(spikes)\n",
    "    for i in range(n):\n",
    "        spike_count.append(np.sum(binary[i][t_min:t_max]))\n",
    "    if num_bins is None: num_bins = int(np.max(spike_count) - np.min(spike_count))\n",
    "    n, bins, patches = ax.hist(spike_count, num_bins, color = 'blue')\n",
    "    ax.set_title(\"Spike Rate Distribution\", fontsize = 25)\n",
    "    ax.set_xlabel(\"Total Number of Spikes\", fontsize = 22)\n",
    "    ax.set_ylabel(\"Number of Neurons\", fontsize = 22)\n",
    "    return(n,bins)\n",
    "\n",
    "def entropy2(labels, base=None):\n",
    "#\"\"\" Computes entropy of label distribution. \"\"\"\n",
    "\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "\n",
    "    value,counts = np.unique(labels, return_counts = True)\n",
    "    probs = counts / n_labels\n",
    "    n_classes = np.count_nonzero(probs)\n",
    "\n",
    "    if n_classes <= 1:\n",
    "        return 0\n",
    "\n",
    "    ent = 0.\n",
    "\n",
    "  # Compute entropy\n",
    "    base = e if base is None else base\n",
    "    for i in probs:\n",
    "        ent -= i * log(i, base)\n",
    "\n",
    "    return ent/log(n_labels,base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comms = 1 # number ofcommunities to expand\n",
    "spike_rate = 10 # spike rate per commiunity\n",
    "comm_size = [20,20,20,20,20,20]\n",
    "num_neurons = int(sum(comm_size))\n",
    "bin_size = 1000.0 # in frames, in every bin_size, a community activity occurs\n",
    "seconds = len(comm_size)\n",
    "total_duration = int(seconds*bin_size)\n",
    "window_sizes = [25, 50, 75, 100, 200, 250, 500, 750, 1000, 1250, 1500, 2000] # size, in frames, each adjacency matrix correspond to. better to be equal to bin_size \n",
    "standard_dev = 1.2 # for gaussian kernel\n",
    "k = 5 #for jittering the spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = np.zeros((num_neurons,total_duration))\n",
    "master_spike = np.zeros((1,total_duration))\n",
    "master = homogeneous_poisson_process(rate = spike_rate*Hz, t_start = 0.0*ms, t_stop = total_duration*ms, as_array=True) \n",
    "\n",
    "for i,e in enumerate(master):    \n",
    "    master_spike[0][int(e)] = 1\n",
    "for i in range(comm_size[0]):\n",
    "    spikes[i] = jitter(master_spike, 5)\n",
    "\n",
    "comms = []\n",
    "for i in range(1,len(comm_size)):\n",
    "    comms.append([homogeneous_poisson_process(rate = spike_rate*Hz, t_start = 0.0*ms, t_stop = i*bin_size*ms, as_array=True) for j in range(comm_size[i])])\n",
    "\n",
    "neuron_count = comm_size[0]\n",
    "for i,e in enumerate(comms):## this is for \n",
    "    for j,f in enumerate(e):\n",
    "        for k,m in enumerate(f):\n",
    "            spikes[neuron_count+j][int(m)] = 1\n",
    "    neuron_count = neuron_count + len(e)\n",
    "            \n",
    "neuron_count = comm_size[0]\n",
    "for i in range(1,len(comms)+1):\n",
    "    for j in range(neuron_count,neuron_count+comm_size[i]):\n",
    "        for k in np.nonzero(spikes[0][(i*1000):])[0]:\n",
    "            jitt = random.randint(-5,5)\n",
    "            try:spikes[j,(i*1000)+k+jitt] = 1\n",
    "            except:spikes[j,k] = 1\n",
    "    neuron_count = neuron_count + comm_size[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(20,10))\n",
    "ax.imshow(spikes, origin = 'lower', interpolation='nearest', aspect='auto',  extent = [0,total_duration,0,num_neurons])\n",
    "ax.set_title('Spike Trains generated via Poisson Process for 120 synthetic neurons', fontsize= 30)\n",
    "ax.set_xlabel('TIME (in Miliseconds)', fontsize = 20)\n",
    "ax.set_xticks([j*1000 for j in range(int(total_duration/1000)+1)])\n",
    "ax.set_yticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "ax.set_ylabel('Neuron ID', fontsize = 25)\n",
    "ax.set_xlabel('Time (Frames)', fontsize = 20)\n",
    "ax.tick_params(axis = 'both', labelsize = 20)\n",
    "#plt.savefig('/projects/academic/smuldoon/bengieru/baby_analysis/spike_train.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize= (10,10))\n",
    "spike_count(spikes, ax)\n",
    "#plt.savefig('/projects/academic/smuldoon/bengieru/baby_analysis/spike_distribution.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_spikes = {}\n",
    "adjacencies = {}\n",
    "for i, window_size in enumerate(window_sizes):\n",
    "    binned_spikes['%d'%window_size] = bin_time_series(spikes, window_size, gaussian = True, sigma = standard_dev)\n",
    "    layers = int(total_duration/window_sizes[i])\n",
    "    adjacency_matrices = []\n",
    "    for j in range(layers):\n",
    "        adjacency_matrices.append(cross_correlation_matrix(binned_spikes['%d'%window_size][j])[0])\n",
    "    adjacencies['%d'%window_size] = adjacency_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = window_sizes[6]\n",
    "layers = int(total_duration/w)\n",
    "\n",
    "fig,ax = plt.subplots(3, 4, figsize = (42,40))\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        k = ax[i][j].imshow(adjacencies['%d'%w][i*4+j], \n",
    "                                origin = 'lower', \n",
    "                                interpolation='nearest', \n",
    "                                aspect='auto',  \n",
    "                                extent = [0, num_neurons, 0, num_neurons])\n",
    "        ax[i][j].set_title('Adjacency Matrix (Layer %d)'%(i*4+j+1), fontsize = 25)\n",
    "        ax[i][j].set_xticks([k*10 for k in range(int(num_neurons/10)+1)])\n",
    "        ax[i][j].set_yticks([k*10 for k in range(int(num_neurons/10)+1)])\n",
    "        ax[i][j].tick_params(axis = 'both', labelsize = 20)\n",
    "fig.suptitle(\"Window Size = %d\"%w, fontsize=46)\n",
    "cbar = fig.colorbar(k, ax = ax.flat, orientation = 'horizontal')\n",
    "cbar.ax.tick_params(labelsize = 25) \n",
    "#plt.savefig('/projects/academic/smuldoon/bengieru/baby_analysis/adjacency_500.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = window_sizes[7]\n",
    "layers = int(total_duration/w)\n",
    "\n",
    "fig,ax = plt.subplots(2, 4, figsize = (40,32))\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        k = ax[i][j].imshow(adjacencies['%d'%w][i*4+j], \n",
    "                                origin = 'lower', \n",
    "                                interpolation='nearest', \n",
    "                                aspect='auto',  \n",
    "                                extent = [0, num_neurons, 0, num_neurons])\n",
    "        ax[i][j].set_title('Adjacency Matrix (Layer %d)'%(i*4+j+1), fontsize = 25)\n",
    "        ax[i][j].set_xticks([k*10 for k in range(int(num_neurons/10)+1)])\n",
    "        ax[i][j].set_yticks([k*10 for k in range(int(num_neurons/10)+1)])\n",
    "        ax[i][j].tick_params(axis = 'both', labelsize = 20)\n",
    "fig.suptitle(\"Window Size = %d\"%w, fontsize=36)\n",
    "cbar = fig.colorbar(k, ax = ax.flat, orientation = 'horizontal')\n",
    "cbar.ax.tick_params(labelsize = 25) \n",
    "plt.savefig('/projects/academic/smuldoon/bengieru/baby_analysis/adjacency_750.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = window_sizes[8]\n",
    "layers = int(total_duration/w)\n",
    "\n",
    "fig,ax = plt.subplots(2, 3, figsize = (30,30))\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        k = ax[i][j].imshow(adjacencies['%d'%w][i*3+j], \n",
    "                                origin = 'lower', \n",
    "                                interpolation='nearest', \n",
    "                                aspect='auto',  \n",
    "                                extent = [0, num_neurons, 0, num_neurons])\n",
    "        ax[i][j].set_title('Adjacency Matrix (Layer %d)'%(i*3+j+1), fontsize = 25)\n",
    "        ax[i][j].set_xticks([k*10 for k in range(int(num_neurons/10)+1)])\n",
    "        ax[i][j].set_yticks([k*10 for k in range(int(num_neurons/10)+1)])\n",
    "        ax[i][j].tick_params(axis = 'both', labelsize = 20)\n",
    "fig.suptitle(\"Window Size = %d\"%w, fontsize=46)\n",
    "cbar = fig.colorbar(k, ax = ax.flat, orientation = 'horizontal')\n",
    "cbar.ax.tick_params(labelsize = 25) \n",
    "plt.savefig('/projects/academic/smuldoon/bengieru/baby_analysis/adjacency_1000.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters = {}\n",
    "binned_spikes = {}\n",
    "for j,window_size in enumerate(window_sizes):\n",
    "    binned_spikes['%d'%window_size] = bin_time_series(spikes, window_size, gaussian = False) \n",
    "    layers, neurons, windowsize = binned_spikes['%d'%window_size].shape\n",
    "    counter = np.zeros((neurons,layers))\n",
    "    for k in range(len(np.transpose(np.nonzero(binned_spikes['%d'%window_size])))):\n",
    "        l, n ,t = np.transpose(np.nonzero(binned_spikes['%d'%window_size]))[k]\n",
    "        counter[n][l] = counter[n][l] + 1\n",
    "    counters['%d'%window_size] = counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_neurons = {}\n",
    "for i,e in enumerate(window_sizes):\n",
    "    temp = np.zeros(counters['%d'%e].shape)\n",
    "    for n in range(num_neurons):\n",
    "        temp[n] = counters['%d'%e][n]/np.sum(counters['%d'%e][n]) \n",
    "    pdf_neurons['%d'%e] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 1, figsize = (12,12))\n",
    "for j,e in enumerate(window_sizes):\n",
    "    entropy_of_a_neuron = []\n",
    "    for n in range(num_neurons):\n",
    "        entropy_of_a_neuron.append(entropy2(pdf_neurons['%d'%e][n]))\n",
    "    ax.plot(entropy_of_a_neuron, label = 'window size = %d'%e)\n",
    "ax.set_title('Entropy of a neuron', fontsize = 30)\n",
    "ax.set_xlabel('Neuron ID', fontsize = 20)\n",
    "ax.set_ylabel('Entropy', fontsize=20)\n",
    "ax.set_xticks([i*10 for i in range(int(num_neurons/10)+1)])\n",
    "ax.set_yticks([i*0.1 for i in range(11)])\n",
    "\n",
    "\n",
    "ax.tick_params(axis = 'both', labelsize = 15)\n",
    "\n",
    "\n",
    "ax.legend(loc = 'best')\n",
    "plt.savefig('/projects/academic/smuldoon/bengieru/baby_analysis/Entropy_spikes.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 1, figsize = (12,12))\n",
    "mean_entropies = []\n",
    "for j,e in enumerate(window_sizes):\n",
    "    entropy_of_a_neuron = []\n",
    "    for n in range(num_neurons):\n",
    "        entropy_of_a_neuron.append(entropy2(pdf_neurons['%d'%e][n]))\n",
    "    mean_entropies.append(np.mean(entropy_of_a_neuron))\n",
    "\n",
    "ax.plot(np.array(window_sizes)/total_duration, mean_entropies)\n",
    "for i,e in enumerate(np.array(window_sizes)/total_duration):\n",
    "    ax.vlines(e,0,1, linestyle = 'dashed', color = 'r')\n",
    "    ax.text(e, 0.5, 'Window Size = %d'%(int(e*total_duration)), rotation=90, verticalalignment='center', fontsize =25)\n",
    "ax.set_title('Mean Entropies', fontsize= 25)\n",
    "ax.set_xlabel('Window Size / Total Duration', fontsize= 20)\n",
    "ax.set_ylabel('Entropy', fontsize = 20)\n",
    "ax.set_xticks([i*0.1 for i in range(int(10*np.max(np.array(window_sizes)/total_duration))+1)])\n",
    "ax.set_yticks([i*0.1 for i in range(11)])\n",
    "\n",
    "\n",
    "ax.tick_params(axis = 'both', labelsize = 15)\n",
    "plt.savefig('/projects/academic/smuldoon/bengieru/baby_analysis/Mean_entropy_spikes.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
